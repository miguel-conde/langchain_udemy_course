{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Build a Chatbot](https://python.langchain.com/docs/tutorials/chatbot/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hi Bob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 11, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-78695ca5-22fa-4fa5-a8fe-a52c4896464d-0', usage_metadata={'input_tokens': 11, 'output_tokens': 11, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have access to any personal data about you unless you share it with me in this conversation. If you'd like to tell me your name, feel free!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 11, 'total_tokens': 44, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-320af506-2a17-4589-bf99-ae92c6b47cab-0', usage_metadata={'input_tokens': 11, 'output_tokens': 33, 'total_tokens': 44, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Bob. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 33, 'total_tokens': 46, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-964eb8a5-fbd9-423b-8a42-964c9eb5f5ee-0', usage_metadata={'input_tokens': 33, 'output_tokens': 13, 'total_tokens': 46, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi, Bob! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob. How can I help you today, Bob?\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I don't have access to personal data about individuals unless it has been shared with me in the course of our conversation. I don't know your name. If you'd like to share it, feel free to do so!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob. If there’s anything specific you’d like to discuss or ask about, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Async function for node:\n",
    "async def call_model(state: MessagesState):\n",
    "    response = await model.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define graph as before:\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "app = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm sorry, but I don't have access to personal information or any context about you to know your name. If you'd like to share it, feel free!\n"
     ]
    }
   ],
   "source": [
    "# Async invocation:\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ahoy there, Jim! What be a’whatcha need from this salty sea dog today? Arrr!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"Hi! I'm Jim.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ye've told me yer name be Jim, savvy? What else can I help ye with, matey? Arrr!\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "¡Hola, Bob! ¿Cómo puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Hi! I'm Bob.\"\n",
    "language = \"Spanish\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Tu nombre es Bob. ¿Hay algo más con lo que pueda ayudarte?\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't know your name. If you tell me, I can remember it for our conversation!\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"What is my name?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You asked, \"What's 2 + 2?\"\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "query = \"What math problem did I ask?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Hi| Todd|!| Here|’s| a| joke| for| you|:\n",
      "\n",
      "|Why| don|’t| skeleton|s| fight| each| other|?\n",
      "\n",
      "|Because| they| don|’t| have| the| guts|!||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"Hi I'm Todd, please tell me a joke.\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
